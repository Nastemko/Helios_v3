"""Tutor service orchestrating LLM translation suggestions."""
from __future__ import annotations

import json
import logging
import re
import textwrap
from dataclasses import dataclass
from typing import Any, Dict, Optional

from pydantic import BaseModel, Field, ValidationError

from models.text import Text, TextSegment
from services.llm import LLMProvider, get_llm_provider

logger = logging.getLogger(__name__)

CODE_FENCE_REGEX = re.compile(r"^```(?:json)?\s*(.*?)\s*```$", re.DOTALL | re.IGNORECASE)


class TutorServiceError(Exception):
    """Raised when the tutor service cannot fulfill a request."""

    def __init__(self, message: str, status_code: int = 500) -> None:
        super().__init__(message)
        self.status_code = status_code


class TranslationSuggestion(BaseModel):
    """Normalized response returned to API consumers."""

    translation: str = Field(..., description="LLM-generated translation suggestion")
    literal_gloss: Optional[str] = Field(
        None, description="Optional literal gloss to help the student"
    )
    rationale: str = Field(
        ..., description="Explanation describing key syntax/semantic choices"
    )
    confidence: float = Field(
        0.6, ge=0.0, le=1.0, description="LLM self-rated confidence"
    )
    source_language: str = Field(..., description="Language of the original text")
    segment_reference: str = Field(..., description="Reference for the surrounding text")
    context_excerpt: str = Field(
        ..., description="Excerpt of the surrounding context shown to the student"
    )
    metadata: dict[str, Any] = Field(
        default_factory=dict,
        description="Echo of metadata supplied by the client along with debug info",
    )


class TutorLLMResponse(BaseModel):
    """Expected payload generated by the LLM."""

    translation: str
    rationale: str
    literal_gloss: Optional[str] = None
    confidence: Optional[float] = None
    notes: Optional[str] = None


@dataclass
class TutorRequestContext:
    text: Text
    segment: TextSegment
    selection: str
    translation_draft: Optional[str]
    language: str
    metadata: Dict[str, Any]


class TutorService:
    """High-level service responsible for translating tutor requests into LLM calls."""

    MAX_SELECTION_CHARS = 600
    MAX_CONTEXT_CHARS = 1200

    SYSTEM_PROMPT = (
        "You are Helios, a helpful and precise tutor helping students translate "
        "Classical Greek and Latin. Focus on clarity, concision, and grammar cues."
    )

    def __init__(self, llm_provider: Optional[LLMProvider] = None) -> None:
        self._llm_provider = llm_provider or get_llm_provider()

    async def suggest_translation(
        self,
        *,
        text: Text,
        segment: TextSegment,
        selection: str,
        translation_draft: Optional[str],
        language: str,
        metadata: Dict[str, Any],
    ) -> TranslationSuggestion:
        if not selection.strip():
            raise TutorServiceError("Selection is empty", status_code=400)

        context = TutorRequestContext(
            text=text,
            segment=segment,
            selection=self._prepare_selection(selection),
            translation_draft=self._prepare_translation_draft(translation_draft),
            language=language,
            metadata=metadata or {},
        )

        prompt = self._build_prompt(context)

        try:
            llm_response = await self._llm_provider.suggest_translation(
                prompt, system_prompt=self.SYSTEM_PROMPT
            )
        except Exception as exc:  # pragma: no cover - network errors
            logger.exception("LLM provider error: %s", exc)
            raise TutorServiceError("Unable to reach LLM provider", status_code=502)

        suggestion = self._parse_llm_response(
            raw_response=llm_response,
            context=context,
        )

        return suggestion

    def _prepare_selection(self, selection: str) -> str:
        cleaned = " ".join(selection.split())
        return cleaned[: self.MAX_SELECTION_CHARS].strip()

    def _prepare_translation_draft(self, draft: Optional[str]) -> Optional[str]:
        if not draft:
            return None
        cleaned = " ".join(draft.split())
        return cleaned[: self.MAX_SELECTION_CHARS].strip()

    def _prepare_context_excerpt(self, segment: TextSegment) -> str:
        content = " ".join(segment.content.split())
        return content[: self.MAX_CONTEXT_CHARS].strip()

    def _build_prompt(self, context: TutorRequestContext) -> str:
        context_excerpt = self._prepare_context_excerpt(context.segment)
        metadata_lines = [
            f"- {key}: {value}"
            for key, value in context.metadata.items()
            if value is not None and value != ""
        ]
        metadata_section = "\n".join(metadata_lines) if metadata_lines else "None."

        translation_hint = (
            context.translation_draft
            if context.translation_draft
            else "No student translation provided."
        )

        template = f"""
You will receive Classical text metadata and a short passage that a student selected.
Return a JSON object with `translation`, `literal_gloss`, `rationale`, and `confidence`.
Keep the translation clear and <= 280 characters.

TEXT METADATA:
- Author: {context.text.author}
- Title: {context.text.title}
- Segment reference: {context.segment.reference}
- Source language: {context.language}

STUDENT CONTEXT (optional info from UI):
{metadata_section}

SELECTED PASSAGE:
{context.selection}

SEGMENT CONTEXT:
{context_excerpt}

STUDENT TRANSLATION ATTEMPT:
{translation_hint}

JSON RESPONSE FORMAT:
{{
  "translation": "...",
  "literal_gloss": "... or null",
  "rationale": "Explain key grammar and vocabulary choices.",
  "confidence": 0.0-1.0
}}
"""
        return textwrap.dedent(template).strip()

    def _parse_llm_response(
        self,
        *,
        raw_response: str,
        context: TutorRequestContext,
    ) -> TranslationSuggestion:
        cleaned = raw_response.strip()

        fence_match = CODE_FENCE_REGEX.match(cleaned)
        if fence_match:
            cleaned = fence_match.group(1).strip()

        # Some models prefix with 'json'
        if cleaned.lower().startswith("json"):
            cleaned = cleaned[4:].strip()

        try:
            payload = json.loads(cleaned)
        except json.JSONDecodeError:
            logger.warning("Failed to parse JSON from LLM, returning raw text")
            return TranslationSuggestion(
                translation=cleaned,
                literal_gloss=None,
                rationale="Raw response from LLM (unable to parse structured JSON).",
                confidence=0.4,
                source_language=context.language,
                segment_reference=context.segment.reference,
                context_excerpt=self._prepare_context_excerpt(context.segment),
                metadata={**context.metadata, "raw_response": cleaned},
            )

        try:
            normalized = TutorLLMResponse.model_validate(payload)
        except ValidationError as exc:
            logger.warning("LLM response schema mismatch: %s", exc)
            return TranslationSuggestion(
                translation=str(payload),
                literal_gloss=None,
                rationale="LLM response did not match expected schema; returning raw payload.",
                confidence=0.4,
                source_language=context.language,
                segment_reference=context.segment.reference,
                context_excerpt=self._prepare_context_excerpt(context.segment),
                metadata={**context.metadata, "raw_response": payload},
            )

        confidence = normalized.confidence if normalized.confidence is not None else 0.65
        confidence = max(0.0, min(1.0, confidence))

        metadata = dict(context.metadata)
        if normalized.notes:
            metadata["tutor_notes"] = normalized.notes

        metadata.setdefault("tutor_version", "v1")

        return TranslationSuggestion(
            translation=normalized.translation.strip(),
            literal_gloss=normalized.literal_gloss.strip()
            if normalized.literal_gloss
            else None,
            rationale=normalized.rationale.strip(),
            confidence=confidence,
            source_language=context.language,
            segment_reference=context.segment.reference,
            context_excerpt=self._prepare_context_excerpt(context.segment),
            metadata=metadata,
        )


_tutor_service: Optional[TutorService] = None


def get_tutor_service() -> TutorService:
    """FastAPI dependency to obtain a singleton TutorService."""
    global _tutor_service
    if _tutor_service is None:
        _tutor_service = TutorService()
    return _tutor_service


def override_tutor_service(service: Optional[TutorService]) -> None:
    """Allow tests to swap the default TutorService instance."""
    global _tutor_service
    _tutor_service = service

